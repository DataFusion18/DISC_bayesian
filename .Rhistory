}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 20)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
library(rvest)
getReviews <- function(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm", page_num=1){
if (page_num <= 1){
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = length(reviews))
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[x] <- current
print(paste(current))
}
return(review_data)
} else {
url_list <- vector(mode = "character", length = page_num)
for (i in 1:page_num){
if(i>1){
url_list[i] <- paste(substr(x=url,start=1,stop=(nchar(url)-4)), "_P",as.character(i),".htm", sep="")
} else{
url_list[i] <- url
}
}
counter <- 1
for (y in 1:page_num){
url <- url_list[y]
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = page_num*10)
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
#print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 20)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
library(rvest)
getReviews <- function(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm", page_num=1){
if (page_num <= 1){
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = length(reviews))
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[x] <- current
print(paste(current))
}
return(review_data)
} else {
url_list <- vector(mode = "character", length = page_num)
for (i in 1:page_num){
if(i>1){
url_list[i] <- paste(substr(x=url,start=1,stop=(nchar(url)-4)), "_P",as.character(i),".htm", sep="")
} else{
url_list[i] <- url
}
}
counter <- 1
review_data <- vector(mode = "character", length = page_num*10)
for (y in 1:page_num){
url <- url_list[y]
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
#print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 20)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
library(rvest)
getReviews <- function(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm", page_num=1){
if (page_num <= 1){
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = length(reviews))
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[x] <- current
print(paste(current))
}
return(review_data)
} else {
url_list <- vector(mode = "character", length = page_num)
for (i in 1:page_num){
if(i>1){
url_list[i] <- paste(substr(x=url,start=1,stop=(nchar(url)-4)), "_P",as.character(i),".htm", sep="")
} else{
url_list[i] <- url
}
}
counter <- 1
review_data <- vector(mode = "character", length = page_num*10)
for (y in 1:page_num){
url <- url_list[y]
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
reviews <- gsub("Advice to Management", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
#print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 430)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Transdev-Reviews-E413452.htm",page_num = 24)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Transdev.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/First-Student-Reviews-E16694.htm",page_num = 44)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/First-Student-Reviews-E16694.htm",page_num = 44)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/First_student.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm",page_num = 63)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Veolia.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Harvard-Pilgrim-Reviews-E2816.htm",page_num = 10)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Harvard_pilgrim.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/TD-Reviews-E3767.htm",page_num = 144)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/TD_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Motorola-Solutions-Reviews-E427189.htm",page_num = 68)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Motorola.csv',encoding="UTF-8")
write.csv(review_data, file = con)
install.packages("gjam")
library(gjam)
install.packages("gjam")
install.packages("RcppArmadillo")
install.packages(gjam)
install.packages("gjam")
install.packages("RcppArmadillo")
install.packages("gjam")
library(gjam)
install.packages("coda")
install.packages("gridExtra")
install.packages('mgcv')
install.packages('zoo')
install.packages('Matrix')
install.packages('cowplot')
install.packages('spBayes')
install.packages('snow')
install.packages('snowfall')
install.packages('rlecuyer')
char.dat = read.csv("chopperCHAR.csv",header=TRUE)
setwd("~/Documents/Junior_Year/DISC_REU/DISC_bayesian_model/")
char.dat = read.csv("chopperCHAR.csv",header=TRUE)
View(char.dat)
names(char.dat) = c("depth.a","depth.b","age.a","age.b","vol","count")
char.dat$age = with(char.dat, (age.a+age.b)/2)                      # age of sediment core section
char.dat$sed.rate = with(char.dat, (depth.b-depth.a)/(age.b-age.a)) # sedimentation rate
char.dat$char = with(char.dat, (count/vol)*sed.rate)                # charcoal accumulation rate
char.dat$offset = with(char.dat,age.b-age.a)                        # time took for sediment core to form
char.dat$influx = with(char.dat,sed.rate/vol)
lily.lake$age <- round(lily.lake$Date) - 2018 # calculate YBP
lily.lake <- read_excel("bigwoods.xls", sheet = 14, col_names = TRUE)
library(readxl)
lily.lake <- read_excel("bigwoods.xls", sheet = 14, col_names = TRUE)
lily.lake$age <- round(lily.lake$Date) - 2018 # calculate YBP
View(lily.lake)
lily.lake$sed.rate <- with(lily.lake, Depth/5)
View(lily.lake)
require(rbacon)
install.packages('rbacon')
lily.lake$influx <- lily.lake$`Char Flux`
lily.lake <- read_excel("bigwoods.xls", sheet = 14, col_names = TRUE)
lake.dat$age <- with(lily.lake, round(Date) - 2018) # calculate YBP
lake.dat$sed.rate <- with(lily.lake, Depth/5) # NOTE: do not have age of sediment core so we left it fixed
lake.dat$influx <- lily.lake$`Char Flux`
lake.date <- data.frame()
lake.dat$age <- with(lily.lake, round(Date) - 2018) # calculate YBP
lake.dat$sed.rate <- with(lily.lake, Depth/5) # NOTE: do not have age of sediment core so we left it fixed
lake.dat$influx <- lily.lake$`Char Flux`
lake.dat <- data.frame()
lake.dat$age <- with(lily.lake, round(Date) - 2018) # calculate YBP
lake.dat$sed.rate <- with(lily.lake, Depth/5) # NOTE: do not have age of sediment core so we left it fixed
lake.dat$influx <- lily.lake$`Char Flux`
length(lily.lake)
lake.dat <- data.frame() * length(lily.lake$Date)
length(lily.lake$Date)
lake.dat <- data.frame(NA*length(lily.lake$Date))
View(lake.dat)
lake.dat <- data.frame(rep(NA,length(lily.lake$Date)))
lake.dat$age <- with(lily.lake, round(Date) - 2018) # calculate YBP
lake.dat$sed.rate <- with(lily.lake, Depth/5) # NOTE: do not have age of sediment core so we left it fixed
lake.dat$influx <- lily.lake$`Char Flux`
lake.dat <- data.frame(age=rep(NA,length(lily.lake$Date)))
lake.dat$age <- with(lily.lake, round(Date) - 2018) # calculate YBP
lake.dat$sed.rate <- with(lily.lake, Depth/5) # NOTE: do not have age of sediment core so we left it fixed
lake.dat$influx <- lily.lake$`Char Flux`
View(lake.dat)
lake.dat$count <- lily.lake$Count
rm(list=ls())
rm(list=ls())
library(raster)
library(rgdal)
library(sp)
library(maps)
library(ggplot2)
library(ggmap)
setwd("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/Landsat8/LC08_L1TP_015033_20160718_20170222_01_T1")
band1 <- raster("LC08_L1TP_015033_20160718_20170222_01_T1_B1.TIF")
band2 <- raster("LC08_L1TP_015033_20160718_20170222_01_T1_B2.TIF")
band3 <- raster("LC08_L1TP_015033_20160718_20170222_01_T1_B3.TIF")
band4 <- raster("LC08_L1TP_015033_20160718_20170222_01_T1_B4.TIF")
band5 <- raster("LC08_L1TP_015033_20160718_20170222_01_T1_B5.TIF")
band6 <- raster("LC08_L1TP_015033_20160718_20170222_01_T1_B2.TIF")
band7 <- raster("LC08_L1TP_015033_20160718_20170222_01_T1_B2.TIF")
# import dominant species dataset
species <- read.csv("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/DominantSpPerPlot.csv")
species <- species[!is.na(species$easting),]
species <- species[!is.na(species$northing),]
# convert UTM to lat, long
utm.coor.serc <- SpatialPoints(cbind(species$easting,species$northing),
proj4string=CRS("+proj=utm +zone=18"))
# Convert to lat/long
long.lat.coor.serc <- as.data.frame(spTransform(utm.coor.serc,CRS("+proj=longlat")))
species.map <- data.frame(species = species$simplifiedClass,
lat = long.lat.coor.serc$coords.x1,
lon = long.lat.coor.serc$coords.x2,
easting = utm.coor.serc$coords.x1,
northing = utm.coor.serc$coords.x2)
# basic color plot
rgb.stack <- stack(band4, band3, band2)
plotRGB(rgb.stack, stretch ="hist")
# reflection of vegetation (using NDVI)
ndvi <- (band5-band4)/(band5+band4)
# imitate a thermal image
breaks = seq(-0.5,0.5, 0.1)
palette <-  colorRampPalette(c("blue", "white", "red"))(10)
plot(ndvi, breaks=breaks, col=palette)
hist(ndvi)
# plot NDVI data over map of maryland
# first, I need to convert the NDVI points to latitude longitude
ndvi.p <- rasterToPoints(ndvi)
View(ndvi.p)
head(ndvi.p)
head(ndvi.p)
View(species.map)
max(species.map$easting)
min(species.map$easting)
View(species)
rm(list=ls())
library(raster)
library(rgdal)
library(ggplot2)
setwd("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/Landsat8/LC08_L1TP_015033_20160718_20170222_01_T1")
# NDVI points import from bandwidths 4 and 5
band4 <- raster("LC08_L1TP_015033_20160718_20170222_01_T1_B4.TIF")
band5 <- raster("LC08_L1TP_015033_20160718_20170222_01_T1_B5.TIF")
ndvi.p <- rasterToPoints((band5-band4)/(band5 + band4))
utm.coor <- SpatialPoints(cbind(ndvi.p[,1],ndvi.p[,2]),
proj4string=CRS("+proj=utm +zone=18"))
long.lat.coor <- as.data.frame(spTransform(utm.coor,CRS("+proj=longlat")))
ndvi.points <- data.frame(layer=ndvi.p[,3], lat=long.lat.coor$coords.x1, lon=long.lat.coor$coords.x2, easting = utm.coor$coords.x1, northing = utm.coor$coords.x2)
# constrain the ndvi points to the plot region
ndvi.points <- ndvi.points[which(ndvi.points$lat > -76.555 & ndvi.points$lat < -76.537),]
ndvi.points <- ndvi.points[which(ndvi.points$lon > 38.8725 & ndvi.points$lon < 38.879),]
# Species Map Import
species <- read.csv("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/DominantSpPerPlot.csv")
# remove plots with no location, if any
species <- species[!is.na(species$easting),]
species <- species[!is.na(species$northing),]
# convert UTM to lat, long
utm.coor.serc <- SpatialPoints(cbind(species$easting,species$northing),
proj4string=CRS("+proj=utm +zone=18"))
# Convert to lat/long
long.lat.coor.serc <- as.data.frame(spTransform(utm.coor.serc,CRS("+proj=longlat")))
species.map <- data.frame(species[2:9],
lat = long.lat.coor.serc$coords.x1,
lon = long.lat.coor.serc$coords.x2,
easting = utm.coor.serc$coords.x1,
northing = utm.coor.serc$coords.x2)
#remove excess environment vars to free memory
rm(species); rm(utm.coor); rm(long.lat.coor.serc); rm(utm.coor.serc); rm(band4); rm(band5); rm(long.lat.coor); rm(ndvi.p)
# save a user-defined dataset
user.species = "scam" #Enter a species (scam, phau, ivfr, c4, spcy, tyla, dead or bare_water)
curr.species.map <- species.map[which(species.map[,user.species]%in%c(NA, 0, 4,5)),c(which(names(species.map)%in%user.species),9:12)]
#graph current species under landsat plots
ggplot() +
geom_rect(data=curr.species.map, aes(xmin=(easting - min(ndvi.points$easting))/30, xmax=(easting - min(ndvi.points$easting) + 20)/30, ymin=(northing -min(ndvi.points$northing))/30, ymax=(northing -min(ndvi.points$northing) + 20)/30, fill = as.factor(unlist(curr.species.map[1]))), color=NA) +
labs(title=paste(user.species,"'s Population Abundance"), x="X (30m increment)", y="Y (30m increment)", fill = "Cover") +
geom_rect(data=ndvi.points, aes(xmin=(easting - min(ndvi.points$easting))/30, xmax=(easting - min(ndvi.points$easting) + 30)/30, ymin=(northing-min(ndvi.points$northing))/30, ymax=(northing + 30 - min(ndvi.points$northing))/30), color="black", fill = NA)
rm(curr.species.map)
# Compiled Files of Charcoal Process Point Model
# Aidan Draper
rm(list=ls())
# import libraries
library(spBayes)
library(ggplot2)
library(coda)
library(gridExtra)
library(mgcv)
library(zoo)
library(Matrix)
library(stringr)
library(snow)
library(rlecuyer)
library(snowfall)
#STEP 1: Setup and Data Manipulation
setwd("~/Documents/Junior_Year/DISC_REU/DISC_bayesian_model/")
char.dat = read.csv("chopperCHAR.csv",header=TRUE)
names(char.dat) = c("depth.a","depth.b","age.a","age.b","vol","count")
head(char.dat)
# create covariates to use in model                                 covariates:
char.dat$age = with(char.dat, (age.a+age.b)/2)                      # age of sediment core section
char.dat$sed.rate = with(char.dat, (depth.b-depth.a)/(age.b-age.a)) # sedimentation rate
char.dat$char = with(char.dat, (count/vol)*sed.rate)                # charcoal accumulation rate
char.dat$offset = with(char.dat,age.b-age.a)                        # time took for sediment core to form
char.dat$influx = with(char.dat,sed.rate/vol)                       # sediment influx rate?
# Drop last row of data with no age information
char.dat = char.dat[-nrow(char.dat),]
# plot 1: charcoal count over time
qplot(age,count,data=char.dat) + geom_smooth(method="loess", span=0.08) + theme_bw()
#STEP 2: Approximate background and foreground intensity (using cubic base spline) and set offset
char.dat$age.c = with(char.dat,age-min(age))
char.dat$age.s = with(char.dat,age.c/max(age.c))
n = nrow(char.dat)
n.knots = 101 # this variable will change for each model based on time interval
# note: smoothCon is part of the mgcv package for constructing the smooth terms in a GAM model
CRbasis = smoothCon(s(age.s,k=n.knots,bs="cr"),data=char.dat,knots=NULL,absorb.cons=TRUE,
scale.penalty=TRUE)
Sb = CRbasis[[1]]$S[[1]]
X = CRbasis[[1]]$X
knots = as.numeric(CRbasis[[1]]$xp)
S.scale = CRbasis[[1]]$S.scale
TT = char.dat$offset # sets offset (time interval between a and b calculated in STEP 1)
#STEP 3: Generate Starting Values (PRIOR?) and fit the first GAM models (m1 and m2) for Background and Foreground
# note: (rollapply is part of the zoo package) applies "quantile(x,0.9)" to get a rolling 90% threshold as more charcoal counts are included
# note: (na.approx is part of the zoo package) approximates the missing values
char.dat$t.hold = rollapply(char.dat$count,50,function(x)quantile(x,0.9),fill="extend")
# Separate charcoal counts
y.back = y.fore = char.dat$count
y.back[char.dat$count>char.dat$t.hold] = NA # removes count value for background if the value is distributed in the 90% quantile
y.fore[char.dat$count<char.dat$t.hold] = NA # removes count value for background if the value is NOT distributed in the 90% quantile
#approximate the missing values (background = na.approx, foreground = randomly pulls value from binomial distribution)
y.back = round(na.approx(y.back))
y.fore[is.na(y.fore)] = rbinom(sum(is.na(y.fore)),1,0.1) # ask about why we add a bunch of zeros instead of NAs
# Plot background and foreground counts
plot(char.dat$age,y.back)
plot(char.dat$age,y.fore)
# Add separate counts to data.frame
char.dat$count.b = y.back
char.dat$count.f = y.fore
# Fit GAM Model to background (gam is part of mgcv package)
m1 = gam(y.back~X,family=poisson,offset=log(TT),paraPen=list(X=list(Sb,sp=0.001*S.scale)))
# summary(m1)
# plots the points and our fitted GAM model for background
plot(char.dat$age,y.back,ylim=c(0,100))
lines(char.dat$age,fitted(m1),col="blue",lwd=2)
# Fit GAM Model to foreground
m2 = gam(y.fore~X,family=poisson,offset=log(TT),paraPen=list(X=list(Sb,sp=(1e-7)*S.scale)))
# summary(m2)
# plots the points and our fitted GAM model for foreground
plot(char.dat$age,y.fore)
lines(char.dat$age,fitted(m2),col="blue",lwd=2)
#STEP 4: Create your holdout set (25% of your data) -
# uses gridded search to find both variances and decide holdout points
ho.idx = rep(NA,round(0.25*n)) # empty vector of 25 percent of data
ho.idx[1] = sample(1:n,1)
exclude = ho.idx[1] + c(-2,-1,0,1,2)
# decides what charcoal counts to exclude
for(i in 2:length(ho.idx)){
check = TRUE
while (check == TRUE){
tmp = sample(1:n,1)
check = tmp %in% exclude
}
ho.idx[i] = tmp
exclude = c(exclude,tmp + c(-2,-1,0,1,2)) # will not exclude points +/- 2 of point
}
# sorts the indexed hold out points
ho.idx = sort(ho.idx)
# plots charcoal count over time with red holdout points on top
plot(char.dat$age,char.dat$count,pch=20,xlim=rev(range(char.dat$age)))
points(char.dat$age[ho.idx],char.dat$count[ho.idx],pch=20,col="red",cex=1.2)
legend(x=500,y=250,legend=c("Held Out Count"),pch=c(20),col=c("red"))
# distribution of the difference between two holdout points
hist(diff(ho.idx))
# records the number of holdout points and new dataset length
n.hold = length(ho.idx)
n.obs = n - n.hold
# resets parameters
char.obs = char.dat[-ho.idx,] # creates new dataset for model
char.ho = char.dat[ho.idx,] # creates a dataset of our holdout points to check our model
y.obs = char.obs$count
y.oos = char.ho$count
X.obs = X[-ho.idx,]
X.ho = X[ho.idx,]
TT.obs = char.obs$offset
TT.ho = char.ho$offset
#STEP 5: Cross-validate your model against the holdout points to see the model's accuracy
# Define number of chains
q = 3
# Define background and foreground penalties (based on the smoothing scale of the splines)
n.pen = 5
sig2.b = seq(100,500,length.out=n.pen)/S.scale
sig2.f = seq(1e8,5e8,length.out=n.pen)/S.scale
pen.params = expand.grid(sig2.b=sig2.b,sig2.f=sig2.f)
G = nrow(pen.params)
# Define run index
run.idx = expand.grid(sig2.b=sig2.b,sig2.f=sig2.f,chain=1:q) # duplicate your (sigmoid function) penalties by the number of chains
run.idx = run.idx[,c("chain","sig2.b","sig2.f")]             # rearrange columns
run.idx$id = sapply(1:nrow(run.idx),function(i,x=run.idx)    # create your id variable
paste(str_pad(which((1:q)%in%x$chain[i]),2,pad="0"),
str_pad(which(sig2.b%in%x$sig2.b[i]),2,pad="0"),
str_pad(which(sig2.f%in%x$sig2.f[i]),2,pad="0"),sep=""))
# FUNCTION (for later): Define log target density function
ld = function(theta,inputs,priors){
for (i in 1:length(inputs)){
tmp = names(inputs)
assign(tmp[i],inputs[[i]])
}
for (i in 1:length(priors)){
tmp = names(priors)
assign(tmp[i],priors[[i]])
}
b0.b = theta[b0.b.idx]
beta.b = theta[beta.b.idx]
b0.f = theta[b0.f.idx]
beta.f = theta[beta.f.idx]
lam.Tb = exp(b0.b*one + X%*%beta.b)*TT
lam.Tf = exp(b0.f*one + X%*%beta.f)*TT
lam.T = lam.Tb + lam.Tf
ld = sum(y*log(lam.T)) - sum(lam.T) -
(1/(2*sig2.0))*crossprod(b0.b-mu.0) -
(1/(2*sig2.b))*crossprod(beta.b-mu.b,S%*%(beta.b-mu.b)) -
(1/(2*sig2.0))*crossprod(b0.f-mu.0) -
(1/(2*sig2.f))*crossprod(beta.f-mu.f,S%*%(beta.f-mu.f))
return(ld)
}
# Setup cross-validation inputs as a large list and save is to an .rda file
# create a list of the parameter names
param.names = c("b0.b",paste("beta.b[",seq(n.knots-1),"]",sep=""),"b0.f",
paste("beta.f[",seq(n.knots-1),"]",sep=""))
# get index of column title
b0.b.idx = which(param.names=="b0.b")
b0.f.idx = which(param.names=="b0.f")
# get indecies where there is not a "column title"
beta.b.idx = which(param.names%in%paste("beta.b[",seq(n.knots-1),"]",sep=""))
beta.f.idx = which(param.names%in%paste("beta.f[",seq(n.knots-1),"]",sep=""))
mod.inputs = list() # creates new list
for (j in 1:q){ # for each chain
for (g in 1:G){ # and for each penalty value
# idx = i + K*(g-1) + K*G*(j-1)
idx = g + G*(j-1) # index is equal to the iterator (indexer) in the run.idx var - weird way to do this IMO
# Define inputs - they all are the CRBasis (cubic spline results) from the oberservation model?
y = y.obs
TT = TT.obs
one = rep(1,n.obs) # vector of 1s for n.obs (which is length(dataset) - length(holdout set))
X = X.obs
inputs = list(y=y,S=Sb,one=one,X=X,TT=TT,b0.b.idx=b0.b.idx,beta.b.idx=beta.b.idx,
b0.f.idx=b0.f.idx,beta.f.idx=beta.f.idx)
#### Define priors ####
priors = list(mu.0=0,sig2.0=1e7,mu.b=rep(0,n.knots-1),mu.f=rep(0,n.knots-1),
sig2.b=pen.params[g,"sig2.b"],sig2.f=pen.params[g,"sig2.f"])
# Define starting values
b0.b = coef(m1)[1] + runif(1,-0.01,0.01)
beta.b = coef(m1)[2:n.knots] + runif(n.knots-1,-0.01,0.01)
b0.f = coef(m2)[1] + runif(1,-0.01,0.01)
beta.f = coef(m2)[2:n.knots] + runif(n.knots-1,-0.01,0.01)
starting = c(b0.b,beta.b,b0.f,beta.f)
mod.inputs[[idx]] = list(ltd=ld,inputs=inputs,priors=priors,tune=0.1,
starting=starting)
}
}
names(mod.inputs) = paste("run",run.idx$id,sep="")
save(mod.inputs,file="InputsCVchopper.rda")
sink("lake_inputs.txt")
print(mod.inputs)
sink()
mydata <- read.table("lakes.txt")
mydata <- read.table("~/Documents/Junior_Year/DISC_REU/DISC_bayesian_model/lakes.txt")
mydata <- read.table("~/Documents/Junior_Year/DISC_REU/DISC_bayesian_model/lake_inputs.txt")
mydata <- read.table("~/Documents/Junior_Year/DISC_REU/DISC_bayesian_model/lake_inputs.txt", fill=TRUE, stringAsFactors=FALSE)
mydata <- read.table("~/Documents/Junior_Year/DISC_REU/DISC_bayesian_model/lake_inputs.txt", fill=TRUE, stringsAsFactors=FALSE)
View(mydata)
